# -*- coding: utf-8 -*-
"""DatasetPrep_sqlAgent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ArzrCq-MDPHHNhupWAwvWECJOLR7I_9s

Heading:
üìÇ CSV to SQLite & ChromaDB Embedding Pipeline

Description:
This notebook:

Mounts Google Drive and locates CSV datasets.

Stores each CSV as a SQLite table for structured queries.

Sets up a persistent ChromaDB vector store.

Uses the BAAI/bge-m3 model to embed text columns (GPU-accelerated).

Saves embeddings with metadata into ChromaDB for semantic search.

Outcome:
Structured SQL database + vectorized text embeddings ready for hybrid RAG + SQL queries.
"""

# Install necessary packages
!pip install chromadb transformers torch tqdm --quiet

import os
import pandas as pd
import sqlite3
import torch
import chromadb
from transformers import AutoTokenizer, AutoModel
from google.colab import drive
from tqdm import tqdm

# ==============================
# 0Ô∏è‚É£ Mount Google Drive
# ==============================
drive.mount('/content/drive', force_remount=True)

# ==============================
# 1Ô∏è‚É£ Define paths
# ==============================
DATA_DIR = "/content/drive/MyDrive/archive"
DB_PATH = os.path.join(DATA_DIR, "olist_sqlite.db")
CHROMA_DB_PATH = os.path.join(DATA_DIR, "chroma_db")  # persistent ChromaDB folder on Drive
os.makedirs(CHROMA_DB_PATH, exist_ok=True)

# ==============================
# 2Ô∏è‚É£ List all CSV files
# ==============================
csv_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith(".csv")]
print("Found CSV files:", csv_files)

# ==============================
# 3Ô∏è‚É£ Store each CSV into SQLite
# ==============================
conn = sqlite3.connect(DB_PATH)
for csv_file in csv_files:
    df = pd.read_csv(os.path.join(DATA_DIR, csv_file))
    table_name = os.path.splitext(csv_file)[0]
    df.to_sql(table_name, conn, if_exists="replace", index=False)
    print(f"‚úÖ Stored {csv_file} as table '{table_name}' in SQLite")
conn.close()

# ==============================
# 4Ô∏è‚É£ Setup persistent ChromaDB client
# ==============================
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

# ==============================
# 5Ô∏è‚É£ Define embedding model wrapper with GPU batching
# ==============================
class BGEM3FlagModel:
    def __init__(self, model_name, use_fp16=False):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        if use_fp16:
            self.model.half()
        self.model.eval()
        if torch.cuda.is_available():
            self.model = self.model.to("cuda")

    def encode(self, texts, batch_size: int = 16, max_length: int = 512):
        all_embeddings = []
        for i in tqdm(range(0, len(texts), batch_size), desc="Embedding batches"):
            batch = texts[i:i+batch_size]
            inputs = self.tokenizer(batch,
                                    return_tensors="pt",
                                    truncation=True,
                                    padding=True,
                                    max_length=max_length)
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda") for k, v in inputs.items()}
            with torch.no_grad():
                output = self.model(**inputs)
            # Mean pooling
            emb = output.last_hidden_state.mean(dim=1).cpu().numpy()
            all_embeddings.extend(emb.tolist())
        return all_embeddings

embedding_model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

# ==============================
# 6Ô∏è‚É£ Embed text columns and store in ChromaDB safely
# ==============================
CHROMA_BATCH_SIZE = 500  # must be <= max batch size of ChromaDB

for csv_file in csv_files:
    df = pd.read_csv(os.path.join(DATA_DIR, csv_file))
    table_name = os.path.splitext(csv_file)[0]

    # Create or get collection
    existing_collections = [c.name for c in chroma_client.list_collections()]
    if table_name in existing_collections:
        collection = chroma_client.get_collection(table_name)
    else:
        collection = chroma_client.create_collection(name=table_name)

    # Identify text/object columns
    text_cols = df.select_dtypes(include="object").columns.tolist()
    if not text_cols:
        print(f"‚ö†Ô∏è Skipping {csv_file}: no text columns")
        continue

    # Concatenate text columns
    texts_to_embed = df[text_cols].astype(str).agg(" ".join, axis=1).tolist()
    ids = df.index.astype(str).tolist()
    metadatas = df.to_dict(orient="records")

    # Compute embeddings in GPU batches
    embeddings = embedding_model.encode(texts_to_embed, batch_size=16, max_length=512)

    # Add to ChromaDB in safe batches
    for i in tqdm(range(0, len(texts_to_embed), CHROMA_BATCH_SIZE), desc=f"Adding '{table_name}' to ChromaDB"):
        batch_texts = texts_to_embed[i:i+CHROMA_BATCH_SIZE]
        batch_embeddings = embeddings[i:i+CHROMA_BATCH_SIZE]
        batch_ids = ids[i:i+CHROMA_BATCH_SIZE]
        batch_metadatas = metadatas[i:i+CHROMA_BATCH_SIZE]

        collection.add(
            documents=batch_texts,
            embeddings=batch_embeddings,
            metadatas=batch_metadatas,
            ids=batch_ids
        )

    print(f"‚úÖ Stored embeddings for '{csv_file}' in ChromaDB collection '{table_name}'")

print("üéâ Stage 1 complete: SQLite at", DB_PATH, "and ChromaDB at", CHROMA_DB_PATH)

import sqlite3

DB_PATH = "/content/drive/MyDrive/archive/olist_sqlite.db"
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()

# List all tables
cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
tables = cursor.fetchall()
conn.close()

print("‚úÖ Tables stored in SQLite:")
for t in tables:
    print(" -", t[0])

import chromadb

CHROMA_DB_PATH = "/content/drive/MyDrive/archive/chroma_db"
client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

collections = client.list_collections()
print("‚úÖ Collections stored in ChromaDB:")
for c in collections:
    print(f" - {c.name}")

for c in collections:
    collection = client.get_collection(c.name)
    count = collection.count()
    print(f"{c.name}: {count} vectors")

"""üß† Interactive SQL Agent with Conversation Memory

This section builds an AI-powered SQL Agent that allows users to query a SQLite database using natural language, without writing SQL manually.
It also maintains conversation context, enabling smooth follow-up questions like "plot that" or "show last query again".
"""

# INTERACTIVE SQL AGENT WITH CONVERSATION MEMORY
import os
import time
from langchain_community.agent_toolkits import create_sql_agent
from langchain_community.utilities import SQLDatabase
from langchain_google_genai import ChatGoogleGenerativeAI

# Configure API
os.environ["GOOGLE_API_KEY"] = ""#please insert your api key here

# Use Gemini 2.0 Flash
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    temperature=0,
    request_timeout=60
)

# Setup Database
sqlite_path = "/content/drive/MyDrive/archive/olist_sqlite.db"
db = SQLDatabase.from_uri(f"sqlite:///{sqlite_path}")

# Create agent
agent = create_sql_agent(
    llm=llm,
    db=db,
    verbose=True,
    handle_parsing_errors=True,
    max_iterations=15,
    max_execution_time=60,
    early_stopping_method="generate"
)

# Conversation memory storage
conversation_history = []

def ask(question: str, delay: float = 2.0) -> str:
    """
    Ask a question with rate limit protection and conversation memory
    """
    # Add delay to avoid rate limits
    time.sleep(delay)

    # Build context from conversation history
    if conversation_history:
        context = "Previous conversation:\n"
        for i, (q, a) in enumerate(conversation_history[-3:], 1):  # Last 3 exchanges
            context += f"\nQ{i}: {q}\nA{i}: {a}\n"
        full_question = f"{context}\n\nCurrent question: {question}"
    else:
        full_question = question

    print(f"\n{'='*60}")
    print(f"üîç Question: {question}")
    print(f"{'='*60}\n")

    try:
        result = agent.invoke({"input": full_question})
        answer = result['output']

        # Store in conversation memory
        conversation_history.append((question, answer))

        print(f"\n{'='*60}")
        print(f"‚úÖ Answer:\n{answer}")
        print(f"{'='*60}\n")

        return answer

    except Exception as e:
        error_msg = str(e)
        if "rate limit" in error_msg.lower() or "quota" in error_msg.lower():
            print(f"\n‚è≥ Rate limit hit. Waiting 60 seconds...\n")
            time.sleep(60)
            return ask(question, delay=5)
        else:
            print(f"\n‚ùå Error: {error_msg}\n")
            return f"Error: {error_msg}"

def show_history():
    """Display conversation history"""
    if not conversation_history:
        print("\nüìù No conversation history yet.\n")
        return

    print("\n" + "="*60)
    print("üìù CONVERSATION HISTORY")
    print("="*60)
    for i, (q, a) in enumerate(conversation_history, 1):
        print(f"\n[{i}] Q: {q}")
        print(f"    A: {a[:200]}..." if len(a) > 200 else f"    A: {a}")
    print("\n" + "="*60 + "\n")

def clear_history():
    """Clear conversation history"""
    global conversation_history
    conversation_history = []
    print("\n‚úÖ Conversation history cleared!\n")

# Display available tables
print("\n" + "="*60)
print("üìä AVAILABLE TABLES")
print("="*60)
for table in db.get_usable_table_names():
    print(f"  ‚Ä¢ {table}")
print("="*60 + "\n")

# Interactive loop
print("ü§ñ SQL Agent is ready! Type your questions below.")
print("Commands: 'history' to view chat, 'clear' to reset, 'quit' to exit\n")

while True:
    try:
        # Get user input
        user_input = input("üí¨ You: ").strip()

        # Handle commands
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("\nüëã Goodbye!\n")
            break

        elif user_input.lower() == 'history':
            show_history()
            continue

        elif user_input.lower() == 'clear':
            clear_history()
            continue

        elif not user_input:
            print("‚ö†Ô∏è  Please enter a question.\n")
            continue

        # Process question
        ask(user_input, delay=2)

    except KeyboardInterrupt:
        print("\n\nüëã Interrupted. Goodbye!\n")
        break
    except Exception as e:
        print(f"\n‚ùå Unexpected error: {e}\n")

# import os
# import time
# from langchain_community.agent_toolkits import create_sql_agent
# from langchain_community.utilities import SQLDatabase
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain.agents.agent_types import AgentType

# # Configure API
# os.environ["GOOGLE_API_KEY"] = ""

# # Use Gemini 2.0 Flash
# llm = ChatGoogleGenerativeAI(
#     model="gemini-2.0-flash",
#     temperature=0,
#     request_timeout=60
# )

# # Setup Database
# sqlite_path = "/content/drive/MyDrive/archive/olist_sqlite.db"
# db = SQLDatabase.from_uri(f"sqlite:///{sqlite_path}")

# # Create agent with BETTER configuration
# agent = create_sql_agent(
#     llm=llm,
#     db=db,
#     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
#     verbose=True,
#     handle_parsing_errors=True,
#     max_iterations=10,
#     max_execution_time=60,
#     return_intermediate_steps=False,
#     agent_executor_kwargs={
#         "handle_parsing_errors": True,
#         "return_intermediate_steps": False
#     }
# )

# # Conversation memory storage
# conversation_history = []

# def ask(question: str, delay: float = 2.0) -> str:
#     """
#     Ask a question with rate limit protection and conversation memory
#     """
#     # Add delay to avoid rate limits
#     time.sleep(delay)

#     # Enhanced prompt to avoid parsing errors
#     enhanced_question = f"""{question}

# IMPORTANT INSTRUCTIONS:
# - When showing individual products, GROUP BY product_id (not by category)
# - Use COUNT(*) or COUNT(product_id) for counting sales
# - Always provide Final Answer in plain text (no markdown, no code blocks)
# - For product IDs, join with product_category_name_translation to show English names
# """

#     # Add conversation context if exists
#     if conversation_history:
#         context = "\nPrevious conversation:\n"
#         for i, (q, a) in enumerate(conversation_history[-2:], 1):
#             context += f"Q{i}: {q}\nA{i}: {a}\n"
#         enhanced_question = context + "\n" + enhanced_question

#     print(f"\n{'='*60}")
#     print(f"üîç Question: {question}")
#     print(f"{'='*60}\n")

#     try:
#         result = agent.invoke({"input": enhanced_question})
#         answer = result['output'] if isinstance(result, dict) else str(result)

#         # Store in conversation memory
#         conversation_history.append((question, answer))

#         print(f"\n{'='*60}")
#         print(f"‚úÖ Answer:\n{answer}")
#         print(f"{'='*60}\n")

#         return answer

#     except Exception as e:
#         error_msg = str(e)

#         # Handle rate limits
#         if "rate limit" in error_msg.lower() or "quota" in error_msg.lower():
#             print(f"\n‚è≥ Rate limit hit. Waiting 60 seconds...\n")
#             time.sleep(60)
#             return ask(question, delay=5)

#         # Handle parsing errors - just return what we can
#         elif "parsing" in error_msg.lower():
#             print(f"\n‚ö†Ô∏è  Parsing error, trying simpler approach...\n")
#             # Try with a simpler, more direct query
#             simple_query = question.replace("names", "product_id and category")
#             return ask(simple_query, delay=3)

#         else:
#             print(f"\n‚ùå Error: {error_msg}\n")
#             return f"Error: {error_msg}"

# def ask_direct_sql(question: str) -> str:
#     """
#     Bypass agent and run SQL directly for problematic queries
#     """
#     import sqlite3
#     import pandas as pd

#     print(f"\n{'='*60}")
#     print(f"üîç Question: {question} (Direct SQL)")
#     print(f"{'='*60}\n")

#     conn = sqlite3.connect(sqlite_path)

#     # Predefined queries for common questions
#     if "top 5" in question.lower() and "product" in question.lower():
#         query = """
#         SELECT
#             oi.product_id,
#             COUNT(*) as sales_count,
#             COALESCE(t.product_category_name_english, p.product_category_name) as category_name
#         FROM olist_order_items_dataset oi
#         INNER JOIN olist_products_dataset p ON oi.product_id = p.product_id
#         LEFT JOIN product_category_name_translation t ON p.product_category_name = t.product_category_name
#         GROUP BY oi.product_id
#         ORDER BY sales_count DESC
#         LIMIT 5
#         """
#     else:
#         conn.close()
#         return "Direct SQL not available for this query. Try ask() function instead."

#     print(f"üìù SQL Query:\n{query}\n")

#     df = pd.read_sql_query(query, conn)
#     conn.close()

#     result = df.to_string(index=False)

#     print(f"\n{'='*60}")
#     print(f"‚úÖ Results:\n{result}")
#     print(f"{'='*60}\n")

#     # Store in memory
#     conversation_history.append((question, result))

#     return result

# def show_history():
#     """Display conversation history"""
#     if not conversation_history:
#         print("\nüìù No conversation history yet.\n")
#         return

#     print("\n" + "="*60)
#     print("üìù CONVERSATION HISTORY")
#     print("="*60)
#     for i, (q, a) in enumerate(conversation_history, 1):
#         print(f"\n[{i}] Q: {q}")
#         print(f"    A: {a[:200]}..." if len(a) > 200 else f"    A: {a}")
#     print("\n" + "="*60 + "\n")

# def clear_history():
#     """Clear conversation history"""
#     global conversation_history
#     conversation_history = []
#     print("\n‚úÖ Conversation history cleared!\n")

# def show_help():
#     """Show help information"""
#     print("\n" + "="*60)
#     print("üí° HELP & TIPS")
#     print("="*60)
#     print("\nüìã Commands:")
#     print("  ‚Ä¢ 'history'  - View conversation history")
#     print("  ‚Ä¢ 'clear'    - Clear conversation history")
#     print("  ‚Ä¢ 'direct'   - Use direct SQL for top 5 products")
#     print("  ‚Ä¢ 'help'     - Show this help")
#     print("  ‚Ä¢ 'quit'     - Exit")

#     print("\nüí¨ Best Prompts:")
#     print("  ‚Ä¢ Show top 5 product_id by sales with category names")
#     print("  ‚Ä¢ What product categories sell the most?")
#     print("  ‚Ä¢ How many orders in total?")
#     print("  ‚Ä¢ Which cities have most customers?")
#     print("\n" + "="*60 + "\n")

# # Display available tables
# print("\n" + "="*60)
# print("üìä AVAILABLE TABLES")
# print("="*60)
# for table in db.get_usable_table_names():
#     print(f"  ‚Ä¢ {table}")
# print("="*60 + "\n")

# show_help()

# print("ü§ñ SQL Agent is ready! Type your questions below.\n")

# # Interactive loop
# while True:
#     try:
#         # Get user input
#         user_input = input("üí¨ You: ").strip()

#         # Handle commands
#         if user_input.lower() in ['quit', 'exit', 'q']:
#             print("\nüëã Goodbye!\n")
#             break

#         elif user_input.lower() == 'history':
#             show_history()
#             continue

#         elif user_input.lower() == 'clear':
#             clear_history()
#             continue

#         elif user_input.lower() == 'help':
#             show_help()
#             continue

#         elif user_input.lower() == 'direct':
#             ask_direct_sql("Show me top 5 product IDs by sales")
#             continue

#         elif not user_input:
#             print("‚ö†Ô∏è  Please enter a question.\n")
#             continue

#         # If asking about top products, offer direct SQL option
#         if "top 5" in user_input.lower() and "product" in user_input.lower():
#             print("\nüí° Tip: Type 'direct' for faster results with direct SQL\n")

#         # Process question
#         ask(user_input, delay=2)

#     except KeyboardInterrupt:
#         print("\n\nüëã Interrupted. Goodbye!\n")
#         break
#     except Exception as e:
#         print(f"\n‚ùå Unexpected error: {e}\n")

!pip install --upgrade langchain langchain-community langchain_google_genai google-generativeai

# ==================== BULLETPROOF INSTALLATION ====================


!pip uninstall -y langchain langchain-community langchain-google-genai google-generativeai -q
!pip install langchain==0.2.16 langchain-community==0.2.16 google-generativeai==0.7.2 chromadb sentence-transformers -q



print("‚úÖ Installation complete!")

!pip install --upgrade pip
!pip install sentence-transformers
!pip install transformers
!pip install numpy --upgrade

import os

chroma_path = "/content/drive/MyDrive/chroma_db_download"

# Make sure the folder exists
os.makedirs(chroma_path, exist_ok=True)

# Check if writable
if os.access(chroma_path, os.W_OK):
    print("‚úÖ Folder is writable")
else:
    print("‚ö†Ô∏è Folder is read-only")

# # ==================== FULL WORKING SQL + RAG AGENT ====================

# import os
# import time
# import sqlite3
# import pandas as pd
# import google.generativeai as genai

# # ==================== CONFIG ====================

# # Google Gemini API
# os.environ["GOOGLE_API_KEY"] = ""  # Replace with your key
# genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
# print("‚úÖ Google AI configured")

# # ==================== IMPORTS ====================

# from langchain_community.utilities import SQLDatabase
# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import Chroma

# print("‚úÖ Imports successful")

# # ==================== DATABASE ====================

# sqlite_path = "/content/drive/MyDrive/archive/olist_sqlite.db"  # Your SQLite DB
# db = SQLDatabase.from_uri(f"sqlite:///{sqlite_path}")
# print("‚úÖ Database connected")

# # ==================== CHROMADB SETUP ====================

# chroma_path = "/content/drive/MyDrive/chroma_db_writeable"
# os.makedirs(chroma_path, exist_ok=True)

# embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# vectorstore = None
# try:
#     vectorstore = Chroma(
#         persist_directory=chroma_path,
#         embedding_function=embedding_model,
#         collection_name="reviews",  # specify collection name
#         anonymized_telemetry=False
#     )
#     vectorstore.persist()  # ensure it's writable
#     print("‚úÖ ChromaDB loaded and writable")
# except Exception as e:
#     vectorstore = None
#     print(f"‚ö†Ô∏è ChromaDB not available: {str(e)[:150]}")

# # ==================== GEMINI MODEL ====================

# model = genai.GenerativeModel('gemini-2.0-flash')
# print("‚úÖ Gemini model ready")

# # ==================== MEMORY ====================

# conversation_history = []

# # ==================== CORE FUNCTIONS ====================

# def generate_sql_from_question(question: str) -> str:
#     schema_info = """
# Tables:
# - olist_order_items_dataset (order_id, product_id, price, freight_value)
# - olist_products_dataset (product_id, product_category_name)
# - olist_orders_dataset (order_id, customer_id, order_status, order_purchase_timestamp)
# - olist_customers_dataset (customer_id, customer_city, customer_state)
# - olist_order_reviews_dataset (review_id, order_id, review_score, review_comment_message)
# - olist_order_payments_dataset (order_id, payment_type, payment_value)
# - product_category_name_translation (product_category_name, product_category_name_english)
# """
#     prompt = f"""You are a SQL query generator. Generate ONLY a valid SQL query.

# Database: {schema_info}

# Question: {question}

# Rules:
# - Start with SELECT (nothing before it)
# - Use COUNT(*) for counting
# - JOIN tables as needed
# - GROUP BY product_id for individual products
# - Use product_category_name_translation for English names

# Generate query (start with SELECT):"""

#     try:
#         response = model.generate_content(
#             prompt,
#             generation_config=genai.GenerationConfig(
#                 temperature=0,
#                 max_output_tokens=500
#             )
#         )
#         sql = response.text.strip()
#         # Clean SQL
#         lines = sql.split('\n')
#         clean_lines = []
#         found_select = False
#         for line in lines:
#             line_upper = line.strip().upper()
#             if line_upper.startswith('SELECT') or found_select:
#                 found_select = True
#                 clean_lines.append(line)
#         sql = '\n'.join(clean_lines).replace('```sql','').replace('```','').strip()
#         if sql.endswith(';'):
#             sql = sql[:-1]
#         return sql
#     except Exception as e:
#         return f"Error generating SQL: {e}"

# def ask(question: str, delay: float = 2.0) -> str:
#     time.sleep(delay)
#     print(f"\n{'='*60}\nüîç Question: {question}\n{'='*60}\n")

#     full_question = question
#     if conversation_history:
#         context = "Previous questions:\n"
#         for q,_ in conversation_history[-2:]:
#             context += f"- {q}\n"
#         full_question = f"{context}\nCurrent: {question}"

#     # Generate SQL
#     print("ü§ñ Generating SQL...")
#     sql = generate_sql_from_question(full_question)
#     if sql.startswith("Error"):
#         print(f"‚ùå {sql}\n")
#         return sql
#     print(f"üìù Generated SQL:\n{sql}\n")

#     # Execute SQL
#     try:
#         conn = sqlite3.connect(sqlite_path)
#         df = pd.read_sql_query(sql, conn)
#         conn.close()
#         result = df.to_string(index=False) if len(df) > 1 else (str(df.iloc[0,0]) if len(df)>0 else "No results found.")

#         # Generate answer via Gemini
#         answer_prompt = f"""Given this SQL query result, provide a clear, natural language answer.

# Question: {question}
# SQL: {sql}
# Result:
# {result}

# Provide a concise, clear answer:"""
#         answer = model.generate_content(answer_prompt).text.strip()
#         conversation_history.append((question, answer))

#         print(f"{'='*60}\n‚úÖ Answer:\n{answer}\n\nüìä Data:\n{result}\n{'='*60}\n")
#         return answer
#     except Exception as e:
#         print(f"‚ùå SQL Error: {e}")
#         return f"SQL Error: {e}"

# # ==================== CHROMADB FUNCTIONS ====================

# def search_reviews(query: str, k: int = 5) -> str:
#     if vectorstore is None:
#         return "‚ùå ChromaDB not available"
#     try:
#         docs = vectorstore.similarity_search(query, k=k)
#         if not docs: return "No reviews found."
#         results = []
#         for i, doc in enumerate(docs,1):
#             cat = doc.metadata.get('category','Unknown')
#             score = doc.metadata.get('score','N/A')
#             results.append(f"[{i}] {cat} | Score: {score}/5\n{doc.page_content[:300]}...\n")
#         result_str = "\n".join(results)
#         print(result_str)
#         return result_str
#     except Exception as e:
#         return f"Error: {str(e)}"

# def analyze_sentiment(category: str) -> str:
#     if vectorstore is None:
#         return "‚ùå ChromaDB not available"
#     try:
#         docs = vectorstore.similarity_search(f"{category} products", k=20)
#         if not docs: return f"No reviews for {category}"
#         pos = sum(1 for d in docs if d.metadata.get('score',3)>=4)
#         neg = sum(1 for d in docs if d.metadata.get('score',3)<=2)
#         neu = len(docs)-pos-neg
#         pos_ex = next((d.page_content[:200] for d in docs if d.metadata.get('score',3)>=4),'None')
#         neg_ex = next((d.page_content[:200] for d in docs if d.metadata.get('score',3)<=2),'None')
#         summary = f"""
# {'='*60}
# SENTIMENT: {category.upper()}
# {'='*60}

# üìà Positive: {pos} reviews
# Example: {pos_ex}

# üìâ Negative: {neg} reviews
# Example: {neg_ex}

# üìä Distribution: {pos} positive | {neg} negative | {neu} neutral
# {'='*60}
# """
#         print(summary)
#         return summary
#     except Exception as e:
#         return f"Error: {str(e)}"

# # Shortcuts
# search = search_reviews
# sentiment = analyze_sentiment

# # ==================== HELPERS ====================

# def show_history():
#     if not conversation_history: print("\nüìù No history\n"); return
#     print("\n" + "="*60 + "\nüìù HISTORY\n" + "="*60)
#     for i,(q,a) in enumerate(conversation_history,1):
#         print(f"[{i}] Q: {q}\n    A: {a[:200]}..." if len(a)>200 else f"[{i}] Q: {q}\n    A: {a}")
#     print("\n" + "="*60 + "\n")

# def clear_history():
#     global conversation_history
#     conversation_history=[]
#     print("\n‚úÖ History cleared\n")

# def show_help():
#     print("\n" + "="*60)
#     print("üí° HELP\n" + "="*60)
#     print("üìã Commands: history | clear | direct | test | help | quit")
#     print("üí¨ SQL Examples: Show top 5 products, total revenue by category, top cities")
#     if vectorstore:
#         print("üîç ChromaDB: search('query') | sentiment('category')")
#     print("\n" + "="*60 + "\n")

# # ==================== STARTUP ====================

# print("\n" + "="*60)
# print("üöÄ SQL + RAG AGENT (Direct Gemini)\n" + "="*60)
# print(f"‚úÖ Database: Connected")
# print(f"{'‚úÖ' if vectorstore else '‚ö†Ô∏è '} ChromaDB: {'Connected' if vectorstore else 'Not available'}")
# print(f"‚úÖ Gemini: 2.0 Flash Direct API")

# print("\nüìä Tables:")
# for table in db.get_usable_table_names():
#     print(f"  ‚Ä¢ {table}")

# show_help()

# # ==================== INTERACTIVE LOOP ====================

# while True:
#     try:
#         user_input = input("üí¨ You: ").strip()
#         if not user_input: continue
#         cmd = user_input.lower()
#         if cmd in ['quit','exit','q']:
#             print("\nüëã Goodbye!\n")
#             break
#         elif cmd=='history': show_history()
#         elif cmd=='clear': clear_history()
#         elif cmd=='help': show_help()
#         elif cmd.startswith('search(') or cmd.startswith('sentiment('):
#             try: eval(user_input)
#             except Exception as e: print(f"‚ùå {e}")
#         else:
#             ask(user_input, delay=2)
#     except KeyboardInterrupt:
#         print("\nüëã Goodbye!\n")
#         break
#     except Exception as e:
#         print(f"‚ùå Error: {e}")

import os

chroma_path = "/content/drive/MyDrive/chroma_db_download"
print("Files in ChromaDB folder:")
for f in os.listdir(chroma_path):
    print(f)

import chromadb

CHROMA_DB_PATH = "/content/drive/MyDrive/chroma_db_download"
client = chromadb.PersistentClient(path=CHROMA_DB_PATH)

collections = client.list_collections()
print("‚úÖ Collections stored in ChromaDB:")
for c in collections:
    print(f" - {c.name}")

# ==================== FULL WORKING SQL + DYNAMIC RAG + GEMINI AGENT ====================

import os
import time
import sqlite3
import pandas as pd
import google.generativeai as genai

# ==================== CONFIG ====================
os.environ["GOOGLE_API_KEY"] = ""  # Replace with your key
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
print("‚úÖ Google AI configured")

# ==================== IMPORTS ====================
from langchain_community.utilities import SQLDatabase
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

print("‚úÖ Imports successful")

# ==================== DATABASE ====================
sqlite_path = "/content/drive/MyDrive/archive/olist_sqlite.db"
db = SQLDatabase.from_uri(f"sqlite:///{sqlite_path}")
print("‚úÖ Database connected")

# ==================== CHROMADB SETUP ====================
chroma_path = "/content/drive/MyDrive/chroma_db_download"
os.makedirs(chroma_path, exist_ok=True)
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

vectorstores = {}
try:
    collections = [f for f in os.listdir(chroma_path) if os.path.isdir(os.path.join(chroma_path, f))]
    if not collections:
        raise ValueError("No collections found in Chroma folder.")
    for coll in collections:
        vectorstores[coll] = Chroma(
            persist_directory=chroma_path,
            embedding_function=embedding_model,
            collection_name=coll
        )
    print(f"‚úÖ ChromaDB loaded. Collections: {list(vectorstores.keys())}")
except Exception as e:
    vectorstores = None
    print(f"‚ö†Ô∏è ChromaDB not available: {str(e)[:150]}")

# ==================== GEMINI MODEL ====================
model = genai.GenerativeModel('gemini-2.0-flash')
print("‚úÖ Gemini model ready")

# ==================== MEMORY ====================
conversation_history = []

# ==================== CORE FUNCTIONS ====================

def generate_sql_from_question(question: str) -> str:
    """Generate SQL query using Gemini"""
    schema_info = """
Tables:
- olist_order_items_dataset (order_id, product_id, price, freight_value)
- olist_products_dataset (product_id, product_category_name)
- olist_orders_dataset (order_id, customer_id, order_status, order_purchase_timestamp)
- olist_customers_dataset (customer_id, customer_city, customer_state)
- olist_order_reviews_dataset (review_id, order_id, review_score, review_comment_message)
- olist_order_payments_dataset (order_id, payment_type, payment_value)
- product_category_name_translation (product_category_name, product_category_name_english)
"""
    prompt = f"""You are a SQL generator. Generate only SQL (SELECT) for the question below. Ensure no SQLite ambiguity errors.
Database: {schema_info}

Question: {question}

Rules:
- Start with SELECT
- Use COUNT(*) for counting
- Join tables as needed
- For product category, use COALESCE(product_category_name_english, product_category_name)
- In GROUP BY, use the full COALESCE expression
- Limit results if needed
Generate SQL (start with SELECT):"""
    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(temperature=0, max_output_tokens=500)
        )
        sql = response.text.strip()
        # Clean SQL
        lines = sql.split('\n')
        clean_lines = []
        found_select = False
        for line in lines:
            line_upper = line.strip().upper()
            if line_upper.startswith('SELECT') or found_select:
                found_select = True
                clean_lines.append(line)
        sql = '\n'.join(clean_lines).replace('```sql','').replace('```','').strip()
        if sql.endswith(';'): sql = sql[:-1]
        return sql
    except Exception as e:
        return f"Error generating SQL: {e}"

def execute_sql(sql: str) -> pd.DataFrame:
    """Execute SQL safely"""
    try:
        conn = sqlite3.connect(sqlite_path)
        df = pd.read_sql_query(sql, conn)
        conn.close()
        return df
    except Exception as e:
        print(f"‚ùå SQL Error: {e}")
        return pd.DataFrame()

# ==================== DYNAMIC CHROMADB FUNCTIONS ====================
def search(collection_name: str, query: str, k: int = 5):
    if not vectorstores or collection_name not in vectorstores:
        return f"‚ùå Collection '{collection_name}' not available."
    docs = vectorstores[collection_name].similarity_search(query, k=k)
    if not docs: return "No results found."
    results = []
    for i, doc in enumerate(docs, 1):
        cat = doc.metadata.get('category','Unknown')
        score = doc.metadata.get('score','N/A')
        results.append(f"[{i}] {cat} | Score: {score}/5\n{doc.page_content[:300]}...\n")
    return "\n".join(results)

def sentiment(collection_name: str, category: str):
    if not vectorstores or collection_name not in vectorstores:
        return f"‚ùå Collection '{collection_name}' not available."
    docs = vectorstores[collection_name].similarity_search(f"{category} products", k=20)
    if not docs: return f"No reviews for {category}"
    pos = sum(1 for d in docs if d.metadata.get('score',3)>=4)
    neg = sum(1 for d in docs if d.metadata.get('score',3)<=2)
    neu = len(docs) - pos - neg
    pos_ex = next((d.page_content[:200] for d in docs if d.metadata.get('score',3)>=4),'None')
    neg_ex = next((d.page_content[:200] for d in docs if d.metadata.get('score',3)<=2),'None')
    summary = f"""
{'='*60}
SENTIMENT: {category.upper()}
{'='*60}

üìà Positive: {pos} reviews
Example: {pos_ex}

üìâ Negative: {neg} reviews
Example: {neg_ex}

üìä Distribution: {pos} positive | {neg} negative | {neu} neutral
{'='*60}
"""
    return summary

# ==================== DYNAMIC AGENT ====================
def ask(question: str, delay: float = 2.0):
    """Decide dynamically whether to use SQL, RAG, or both"""
    time.sleep(delay)
    print(f"\n{'='*60}\nüîç Question: {question}\n{'='*60}\n")

    # Prepare context for Gemini
    full_question = question
    if conversation_history:
        context = "Previous questions:\n" + "\n".join(f"- {q}" for q,_ in conversation_history[-2:])
        full_question = f"{context}\nCurrent: {question}"

    # 1Ô∏è‚É£ Generate SQL
    sql = generate_sql_from_question(full_question)
    df = execute_sql(sql)
    sql_result = df.to_string(index=False) if not df.empty else "No SQL results found."

    # 2Ô∏è‚É£ Generate RAG outputs from all collections
    rag_results = {}
    if vectorstores:
        for coll in vectorstores:
            rag_res = search(coll, question, k=3)
            if rag_res != "No results found.":
                rag_results[coll] = rag_res

    # 3Ô∏è‚É£ Combine results
    combined_output = f"üí° SQL Result:\n{sql_result}\n\n"
    if rag_results:
        combined_output += "üí° RAG Results:\n"
        for coll, res in rag_results.items():
            combined_output += f"[Collection: {coll}]\n{res}\n"

    # 4Ô∏è‚É£ Optional: Generate Gemini natural language summary
    summary_prompt = f"""Provide a concise answer to the user question using both SQL and RAG results.
Question: {question}
SQL Result:
{sql_result}
RAG Result:
{rag_results if rag_results else 'None'}

Provide the answer in clear, human-friendly language:"""
    answer = model.generate_content(summary_prompt).text.strip()
    conversation_history.append((question, answer))

    print(f"{'='*60}\n‚úÖ Answer:\n{answer}\n\nüìä Combined Data:\n{combined_output}\n{'='*60}\n")
    return answer

# ==================== HELPERS ====================
def show_history():
    if not conversation_history: print("\nüìù No history\n"); return
    print("\n" + "="*60 + "\nüìù HISTORY\n" + "="*60)
    for i,(q,a) in enumerate(conversation_history,1):
        print(f"[{i}] Q: {q}\n    A: {a[:200]}..." if len(a)>200 else f"[{i}] Q: {q}\n    A: {a}")
    print("\n" + "="*60 + "\n")

def clear_history():
    global conversation_history
    conversation_history=[]
    print("\n‚úÖ History cleared\n")

def show_help():
    print("\n" + "="*60)
    print("üí° HELP\n" + "="*60)
    print("üìã Commands: history | clear | direct | test | help | quit")
    print("üí¨ SQL Examples: Show top 5 products, total revenue by category, top cities")
    if vectorstores:
        print("üîç ChromaDB usage: search('collection_name','query') | sentiment('collection_name','category')")
    print("\n" + "="*60 + "\n")

# ==================== STARTUP ====================
print("\n" + "="*60)
print("üöÄ SQL + RAG AGENT (Dynamic)\n" + "="*60)
print(f"‚úÖ Database: Connected")
print(f"{'‚úÖ' if vectorstores else '‚ö†Ô∏è '} ChromaDB: {'Connected' if vectorstores else 'Not available'}")
print(f"‚úÖ Gemini: 2.0 Flash Direct API")

print("\nüìä Tables:")
for table in db.get_usable_table_names():
    print(f"  ‚Ä¢ {table}")

show_help()

# ==================== INTERACTIVE LOOP ====================
while True:
    try:
        user_input = input("üí¨ You: ").strip()
        if not user_input: continue
        cmd = user_input.lower()
        if cmd in ['quit','exit','q']:
            print("\nüëã Goodbye!\n")
            break
        elif cmd=='history': show_history()
        elif cmd=='clear': clear_history()
        elif cmd=='help': show_help()
        elif user_input.startswith('search(') or user_input.startswith('sentiment('):
            try: eval(user_input)
            except Exception as e: print(f"‚ùå {e}")
        else:
            ask(user_input, delay=2)
    except KeyboardInterrupt:
        print("\nüëã Goodbye!\n")
        break
    except Exception as e:
        print(f"‚ùå Error: {e}")

!pip uninstall -y langchain langchain-community langchain-google-genai google-generativeai -q
!pip install langchain==0.2.16 langchain-community==0.2.16 google-generativeai==0.7.2 chromadb sentence-transformers -q
!pip install --upgrade pip
!pip install sentence-transformers
!pip install transformers
!pip install numpy --upgrade

!pip install sentence-transformers

!pip uninstall -y numpy sentence-transformers
!pip install numpy==1.26.4 sentence-transformers==2.2.2

pip install sentence-transformers

"""üß† Stage 3: Multi-Agent System ‚Äî SQL + RAG + PLOT + WEB
üéØ Objective

To build a unified AI system that can:

Understand natural language queries

Query structured databases (SQL)

Retrieve semantic information from vector stores (RAG)

Visualize insights automatically (PLOT)

Fetch external information (WEB)
All working together dynamically under one interactive agent loop.
"""

# ==================== MULTI-AGENT SYSTEM: SQL + RAG + PLOT + WEB ====================

import os
import time
import sqlite3
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import google.generativeai as genai
from langchain_community.utilities import SQLDatabase
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import requests
from bs4 import BeautifulSoup

# ==================== CONFIG ====================
os.environ["GOOGLE_API_KEY"] = ""
genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
print("‚úÖ Google AI configured")

# Set plotting style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# ==================== DATABASE ====================
sqlite_path = "/content/drive/MyDrive/archive/olist_sqlite.db"
db = SQLDatabase.from_uri(f"sqlite:///{sqlite_path}")
print("‚úÖ Database connected")

# ==================== SCHEMA INFO ====================
SCHEMA_INFO = """
Tables and Columns:

1. olist_customers_dataset (99,441 rows)
   Columns: customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state

2. olist_order_payments_dataset (103,886 rows)
   Columns: order_id, payment_sequential, payment_type, payment_installments, payment_value

3. olist_order_items_dataset (112,650 rows)
   Columns: order_id, order_item_id, product_id, seller_id, shipping_limit_date, price, freight_value

4. olist_order_reviews_dataset (99,224 rows)
   Columns: review_id, order_id, review_score, review_comment_title, review_comment_message, review_creation_date, review_answer_timestamp

5. olist_sellers_dataset (3,095 rows)
   Columns: seller_id, seller_zip_code_prefix, seller_city, seller_state

6. product_category_name_translation (71 rows)
   Columns: product_category_name, product_category_name_english

7. olist_products_dataset (32,951 rows)
   Columns: product_id, product_category_name, product_name_lenght, product_description_lenght, product_photos_qty, product_weight_g, product_length_cm, product_height_cm, product_width_cm

8. olist_orders_dataset (99,441 rows)
   Columns: order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date

9. olist_geolocation_dataset
   Columns: geolocation_zip_code_prefix, geolocation_lat, geolocation_lng, geolocation_city, geolocation_state

Key Relationships:
- olist_customers_dataset.customer_id ‚Üí olist_orders_dataset.customer_id
- olist_orders_dataset.order_id ‚Üí olist_order_items_dataset.order_id
- olist_orders_dataset.order_id ‚Üí olist_order_reviews_dataset.order_id
- olist_orders_dataset.order_id ‚Üí olist_order_payments_dataset.order_id
- olist_order_items_dataset.product_id ‚Üí olist_products_dataset.product_id
- olist_order_items_dataset.seller_id ‚Üí olist_sellers_dataset.seller_id
- olist_products_dataset.product_category_name ‚Üí product_category_name_translation.product_category_name
"""

# ==================== CHROMADB SETUP ====================
chroma_path = "/content/drive/MyDrive/chroma_db_download"
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

vectorstores = {}
collection_names = [
    "olist_order_reviews_dataset",
    "olist_products_dataset",
    "olist_order_items_dataset",
    "product_category_name_translation"
]

try:
    for coll_name in collection_names:
        try:
            vs = Chroma(
                persist_directory=chroma_path,
                embedding_function=embedding_model,
                collection_name=coll_name
            )
            test = vs.similarity_search("test", k=1)
            if test:
                vectorstores[coll_name] = vs
                print(f"‚úÖ Loaded collection: {coll_name}")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load {coll_name}: {str(e)[:100]}")

    if vectorstores:
        print(f"‚úÖ ChromaDB ready with {len(vectorstores)} collections")
    else:
        vectorstores = None
except Exception as e:
    vectorstores = None
    print(f"‚ùå ChromaDB error: {str(e)[:200]}")

# ==================== GEMINI MODEL ====================
model = genai.GenerativeModel('gemini-2.0-flash')
print("‚úÖ Gemini model ready")

# ==================== MEMORY ====================
conversation_history = []
last_sql_result = None  # Store last SQL result for plotting

# ==================== AGENT ROUTER ====================
def route_question(question: str) -> str:
    """Determines which agent(s) should handle the question"""

    routing_prompt = f"""Analyze this question and determine which agents to use:

Question: {question}

Available Agents:
1. SQL - Query database for sales, orders, products, customers
2. RAG - Search customer reviews and product information
3. PLOT - Create visualizations (bar charts, line graphs, pie charts, etc.)
4. WEB - Search internet for general knowledge, product info, definitions

Rules:
- Use PLOT if: asking to plot/visualize/graph/chart previous results OR asking for visual analysis
- Use WEB if: asking about general knowledge, definitions, what is X, external information
- Use SQL+RAG if: asking about products, sales, reviews (database queries)
- Can combine multiple agents (e.g., SQL+RAG+PLOT, WEB+SQL+RAG)

Previous context exists: {len(conversation_history) > 0}
Last query had data: {last_sql_result is not None}

Return ONLY ONE LINE with comma-separated agents in order of execution:
Examples:
- "SQL,RAG,PLOT"
- "WEB"
- "PLOT"
- "SQL,RAG"
- "WEB,SQL,RAG"

Your response (agents only):"""

    try:
        response = model.generate_content(
            routing_prompt,
            generation_config=genai.GenerationConfig(temperature=0, max_output_tokens=50)
        )
        agents = response.text.strip().upper().split(',')
        agents = [a.strip() for a in agents if a.strip() in ['SQL', 'RAG', 'PLOT', 'WEB']]
        return agents if agents else ['SQL', 'RAG']
    except Exception as e:
        print(f"‚ö†Ô∏è Routing error: {e}, defaulting to SQL+RAG")
        return ['SQL', 'RAG']

# ==================== WEB SEARCH AGENT ====================
def web_search_agent(question: str) -> str:
    """Search the web for information"""
    print(f"üåê WEB AGENT: Searching for '{question}'")

    search_prompt = f"""Based on this question, provide a comprehensive answer using your knowledge:

Question: {question}

If this is about a product category (like 'bed bath table'), explain:
- What products are typically in this category
- Common uses and purposes
- Who typically buys these products

Provide a clear, informative answer:"""

    try:
        response = model.generate_content(search_prompt)
        answer = response.text.strip()
        print(f"‚úÖ WEB AGENT: Found information\n")
        return answer
    except Exception as e:
        return f"‚ùå Web search failed: {e}"

# ==================== PLOTTING AGENT ====================
def plotting_agent(question: str, data: pd.DataFrame = None) -> str:
    """Creates visualizations from data"""
    global last_sql_result

    if data is None:
        data = last_sql_result

    if data is None or len(data) == 0:
        return "‚ùå No data available to plot. Please run a query first."

    print(f"üìä PLOT AGENT: Creating visualization")
    print(f"   Data shape: {data.shape}")
    print(f"   Columns: {list(data.columns)}")

    # Determine plot type using AI
    plot_prompt = f"""Given this data, what type of plot is most appropriate?

Question: {question}

Data columns: {list(data.columns)}
Data shape: {data.shape}
First few rows:
{data.head(3).to_string()}

Choose ONE plot type:
- bar (for comparing categories/discrete values)
- line (for trends over time)
- pie (for proportions, max 10 slices)
- scatter (for correlation between two numeric variables)
- hist (for distribution of single numeric variable)

Also provide:
- x_column: column name for x-axis (or None for index)
- y_column: column name for y-axis
- title: appropriate plot title
- limit: number of rows to plot (max 50 for readability)

Return ONLY valid JSON:
{{"plot_type": "bar", "x_column": "seller_id", "y_column": "sales_frequency", "title": "Top Sellers", "limit": 25}}

Your JSON:"""

    try:
        response = model.generate_content(
            plot_prompt,
            generation_config=genai.GenerationConfig(temperature=0)
        )

        # Parse AI response
        import json
        config_text = response.text.strip()
        config_text = config_text.replace('```json', '').replace('```', '').strip()
        config = json.loads(config_text)

        plot_type = config.get('plot_type', 'bar')
        x_col = config.get('x_column')
        y_col = config.get('y_column')
        title = config.get('title', 'Data Visualization')
        limit = min(config.get('limit', 25), 50)

        # Prepare data
        plot_data = data.head(limit).copy()

        # Create plot
        plt.figure(figsize=(14, 7))

        if plot_type == 'bar':
            if x_col and x_col in plot_data.columns:
                x_data = plot_data[x_col]
                # Truncate long labels
                if plot_data[x_col].dtype == 'object':
                    x_labels = [str(x)[:15] + '...' if len(str(x)) > 15 else str(x) for x in x_data]
                else:
                    x_labels = x_data
            else:
                x_labels = range(len(plot_data))

            y_data = plot_data[y_col] if y_col in plot_data.columns else plot_data.iloc[:, -1]

            plt.bar(x_labels, y_data, color='steelblue', alpha=0.8)
            plt.xlabel(x_col if x_col else 'Index')
            plt.ylabel(y_col if y_col else 'Value')
            plt.xticks(rotation=45, ha='right')

        elif plot_type == 'line':
            x_data = plot_data[x_col] if x_col and x_col in plot_data.columns else range(len(plot_data))
            y_data = plot_data[y_col] if y_col in plot_data.columns else plot_data.iloc[:, -1]
            plt.plot(x_data, y_data, marker='o', linewidth=2, markersize=6)
            plt.xlabel(x_col if x_col else 'Index')
            plt.ylabel(y_col if y_col else 'Value')

        elif plot_type == 'pie':
            labels_col = x_col if x_col and x_col in plot_data.columns else plot_data.columns[0]
            values_col = y_col if y_col in plot_data.columns else plot_data.columns[-1]

            labels = [str(x)[:20] for x in plot_data[labels_col].head(10)]
            values = plot_data[values_col].head(10)

            plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)
            plt.axis('equal')

        elif plot_type == 'scatter':
            if x_col and y_col and x_col in plot_data.columns and y_col in plot_data.columns:
                plt.scatter(plot_data[x_col], plot_data[y_col], alpha=0.6, s=100)
                plt.xlabel(x_col)
                plt.ylabel(y_col)
            else:
                return "‚ùå Scatter plot requires two numeric columns"

        elif plot_type == 'hist':
            col = y_col if y_col in plot_data.columns else plot_data.columns[-1]
            plt.hist(plot_data[col], bins=30, color='skyblue', edgecolor='black', alpha=0.7)
            plt.xlabel(col)
            plt.ylabel('Frequency')

        plt.title(title, fontsize=14, fontweight='bold')
        plt.tight_layout()

        # Save plot
        plot_filename = f"plot_{int(time.time())}.png"
        plt.savefig(plot_filename, dpi=150, bbox_inches='tight')
        print(f"‚úÖ PLOT AGENT: Saved as {plot_filename}\n")
        plt.show()

        return f"‚úÖ Plot created and saved as {plot_filename}"

    except Exception as e:
        print(f"‚ùå PLOT AGENT Error: {e}")
        return f"‚ùå Plotting failed: {e}"

# ==================== SQL GENERATION ====================
def generate_sql_from_question(question: str) -> str:
    prompt = f"""You are a SQL query generator for a Brazilian e-commerce database.

{SCHEMA_INFO}

Question: {question}

Critical Rules:
- Generate ONLY valid SQL (no explanations, no markdown, no ```sql tags)
- Start directly with SELECT (nothing before it)
- Use exact table names from schema above
- Use exact column names (e.g., product_name_lenght NOT product_name_length)
- For product categories in English, JOIN with product_category_name_translation
- For reviews, use olist_order_reviews_dataset
- For sales/revenue, use olist_order_items_dataset.price
- ALWAYS include relevant IDs in SELECT (product_id, order_id, seller_id)
- ALWAYS include product_category_name_english when category is relevant
- Use COUNT(*) for counting records
- Use SUM(price) for revenue calculations
- Use LIMIT for top N queries
- No semicolon at end

Generate query (start with SELECT):"""

    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(temperature=0, max_output_tokens=600)
        )
        sql = response.text.strip()

        # Clean SQL
        lines = sql.split('\n')
        clean_lines = []
        found_select = False
        for line in lines:
            line_upper = line.strip().upper()
            if line_upper.startswith('SELECT') or found_select:
                found_select = True
                clean_lines.append(line)

        sql = '\n'.join(clean_lines).replace('```sql','').replace('```','').strip()
        if sql.endswith(';'):
            sql = sql[:-1]
        return sql
    except Exception as e:
        return f"Error generating SQL: {e}"

# ==================== RAG SEARCH FUNCTIONS ====================
def search_reviews_by_order_ids(order_ids: list, k: int = 5) -> str:
    """Search reviews collection by order_ids from SQL results"""
    if not vectorstores or "olist_order_reviews_dataset" not in vectorstores:
        return ""

    vs = vectorstores["olist_order_reviews_dataset"]
    all_docs = []

    for order_id in order_ids[:5]:
        try:
            docs = vs.similarity_search(str(order_id), k=2)
            for doc in docs:
                if doc.metadata.get('order_id') == order_id:
                    all_docs.append(doc)
        except:
            continue

    if not all_docs:
        try:
            all_docs = vs.similarity_search("customer review feedback quality delivery", k=k)
        except:
            return ""

    results = []
    for i, doc in enumerate(all_docs[:k], 1):
        review_id = doc.metadata.get('review_id', 'Unknown')
        order_id = doc.metadata.get('order_id', 'Unknown')
        score = doc.metadata.get('review_score', 'N/A')
        content = doc.page_content[:300].strip()
        results.append(f"[{i}] Review ID: {review_id[:16]}... | Order: {order_id[:16]}... | Score: {score}/5\n{content}...")

    return "\n\n".join(results)

def search_reviews_by_product_ids(product_ids: list, k: int = 5) -> str:
    """Search reviews by product IDs"""
    if not vectorstores or "olist_order_reviews_dataset" not in vectorstores:
        return ""

    vs = vectorstores["olist_order_reviews_dataset"]

    # Get order_ids for these products from SQL
    try:
        conn = sqlite3.connect(sqlite_path)
        product_ids_str = "','".join([str(pid) for pid in product_ids[:5]])
        query = f"""
        SELECT DISTINCT o.order_id
        FROM olist_order_items_dataset o
        WHERE o.product_id IN ('{product_ids_str}')
        LIMIT 20
        """
        df = pd.read_sql_query(query, conn)
        conn.close()

        if len(df) > 0:
            order_ids = df['order_id'].tolist()
            return search_reviews_by_order_ids(order_ids, k=k)
    except Exception as e:
        print(f"‚ö†Ô∏è Error fetching reviews for products: {e}")

    return ""

def search_reviews_by_product_category(category: str, k: int = 5) -> str:
    """Search reviews by product category"""
    if not vectorstores or "olist_order_reviews_dataset" not in vectorstores:
        return ""

    vs = vectorstores["olist_order_reviews_dataset"]
    search_query = f"{category} product quality delivery customer satisfaction"

    try:
        docs = vs.similarity_search(search_query, k=k*2)
        results = []
        for doc in docs[:k]:
            review_id = doc.metadata.get('review_id', 'Unknown')
            score = doc.metadata.get('review_score', 'N/A')
            content = doc.page_content[:300].strip()
            if content and content != 'nan nan':
                results.append(f"Review ID: {review_id[:16]}... | Score: {score}/5\n{content}...")

        return "\n\n".join(results) if results else ""
    except Exception as e:
        return ""

# ==================== MAIN ASK FUNCTION ====================
def ask(question: str, delay: float = 1.0) -> str:
    global last_sql_result
    time.sleep(delay)
    print(f"\n{'='*70}\nüîç Question: {question}\n{'='*70}\n")

    # Build context
    full_question = question
    if conversation_history:
        context = "Previous context:\n"
        for q, _ in conversation_history[-2:]:
            context += f"- {q}\n"
        full_question = f"{context}\nCurrent: {question}"

    # Route to appropriate agent(s)
    agents = route_question(question)
    print(f"ü§ñ Routing to agents: {' ‚Üí '.join(agents)}\n")

    web_result = ""
    sql_result = ""
    rag_results = ""
    plot_result = ""

    # Execute agents in sequence
    for agent in agents:
        if agent == 'WEB':
            web_result = web_search_agent(question)

        elif agent == 'SQL':
            # Generate and execute SQL
            print("ü§ñ Generating SQL query...")
            sql = generate_sql_from_question(full_question)

            if sql.startswith("Error"):
                print(f"‚ùå {sql}\n")
                return sql

            print(f"üìù Generated SQL:\n{sql}\n")
            print("‚öôÔ∏è  Executing SQL...")

            try:
                conn = sqlite3.connect(sqlite_path)
                df = pd.read_sql_query(sql, conn)
                conn.close()

                if len(df) == 0:
                    return "‚ùå No results found in database."

                last_sql_result = df  # Store for plotting
                sql_result = df.to_string(index=False)
                print(f"‚úÖ SQL Success! Found {len(df)} row(s).\n")

            except Exception as e:
                error_msg = f"‚ùå SQL Error: {e}"
                print(error_msg)
                return error_msg

        elif agent == 'RAG':
            if last_sql_result is not None and len(last_sql_result) > 0:
                print(f"üîç Searching RAG for related reviews...")
                df = last_sql_result

                # Strategy 1: Search by product_ids
                if 'product_id' in df.columns:
                    product_ids = df['product_id'].tolist()
                    print(f"   Found {len(product_ids)} product IDs, searching reviews...")
                    rag_results = search_reviews_by_product_ids(product_ids, k=5)

                # Strategy 2: Search by order_ids
                if not rag_results and 'order_id' in df.columns:
                    order_ids = df['order_id'].tolist()
                    print(f"   Found {len(order_ids)} order IDs, searching reviews...")
                    rag_results = search_reviews_by_order_ids(order_ids, k=5)

                # Strategy 3: Search by category
                if not rag_results and 'product_category_name_english' in df.columns:
                    category = df['product_category_name_english'].iloc[0]
                    print(f"   Searching reviews for category: {category}")
                    rag_results = search_reviews_by_product_category(category, k=5)

                print(f"   {'‚úÖ Found reviews!' if rag_results else '‚ö†Ô∏è No reviews found'}\n")
            else:
                print("‚ö†Ô∏è No SQL data available for RAG search\n")

        elif agent == 'PLOT':
            plot_result = plotting_agent(question)

    # Generate final answer
    if not any([web_result, sql_result, rag_results]):
        return "‚ùå No results from any agent"

    answer_prompt = f"""Synthesize information from multiple agents:

Question: {question}

{'Web Search Results:\n' + web_result + '\n' if web_result else ''}
{'SQL Results:\n' + sql_result + '\n' if sql_result else ''}
{'Customer Reviews (RAG):\n' + rag_results + '\n' if rag_results else ''}
{'Visualization:\n' + plot_result + '\n' if plot_result else ''}

Instructions:
- Provide clear, comprehensive insights
- Integrate information from all available sources
- Include key metrics and patterns
- Translate Portuguese reviews to English
- Be concise and directly answer the question

Your analysis:"""

    try:
        print("ü§ñ Generating final answer...")
        answer = model.generate_content(answer_prompt).text.strip()
    except Exception as e:
        answer = f"Answer generation failed: {e}"

    # Save to memory
    conversation_history.append((question, answer))

    # Output
    print(f"{'='*70}\n‚úÖ FINAL ANSWER:\n{answer}\n")
    if web_result:
        print(f"üåê WEB INFO:\n{web_result}\n")
    if sql_result:
        print(f"üìä SQL RESULT:\n{sql_result}\n")
    if rag_results:
        print(f"üìÑ CUSTOMER REVIEWS:\n{rag_results}\n")
    if plot_result:
        print(f"üìà VISUALIZATION:\n{plot_result}\n")
    print(f"{'='*70}\n")

    return answer

# ==================== HELPER FUNCTIONS ====================
def show_history():
    if not conversation_history:
        print("\nüìù No conversation history\n")
        return
    print("\n" + "="*70 + "\nüìù CONVERSATION HISTORY\n" + "="*70)
    for i, (q, a) in enumerate(conversation_history, 1):
        print(f"\n[{i}] Q: {q}")
        print(f"    A: {a[:300]}..." if len(a) > 300 else f"    A: {a}")
    print("\n" + "="*70 + "\n")

def clear_history():
    global conversation_history, last_sql_result
    conversation_history = []
    last_sql_result = None
    print("\n‚úÖ History cleared\n")

def show_help():
    print("\n" + "="*70)
    print("üí° MULTI-AGENT SYSTEM - HELP\n" + "="*70)
    print("\nü§ñ Available Agents:")
    print("  üìä SQL    - Query database for sales, products, orders")
    print("  üìÑ RAG    - Search customer reviews")
    print("  üìà PLOT   - Create visualizations")
    print("  üåê WEB    - Search internet for general knowledge")
    print("\nüìã Commands:")
    print("  history  - View conversation history")
    print("  clear    - Clear history")
    print("  help     - Show this help")
    print("  quit     - Exit")
    print("\nüí¨ Example Queries:")
    print("\n  üîó Multi-Agent Workflows:")
    print("    ‚Ä¢ Top 5 products by sales and show me reviews")
    print("       ‚Üí SQL finds products ‚Üí RAG gets reviews")
    print("\n    ‚Ä¢ Top 25 sellers and plot it")
    print("       ‚Üí SQL finds sellers ‚Üí PLOT creates bar chart")
    print("\n    ‚Ä¢ What is bed bath table category?")
    print("       ‚Üí WEB explains category ‚Üí SQL shows products")
    print("\n    ‚Ä¢ Best selling product in health_beauty with reviews and example")
    print("       ‚Üí SQL finds product ‚Üí RAG gets reviews ‚Üí WEB explains product type")
    print("\n  üìä Plotting:")
    print("    ‚Ä¢ Can you plot this as a graph?")
    print("    ‚Ä¢ Show me a chart of the previous results")
    print("    ‚Ä¢ Visualize revenue by category")
    print("\n  üåê Web Search:")
    print("    ‚Ä¢ What is bed bath table?")
    print("    ‚Ä¢ Explain health and beauty products")
    print("\n" + "="*70 + "\n")

# ==================== STARTUP ====================
print("\n" + "="*70)
print("üöÄ MULTI-AGENT SYSTEM: SQL + RAG + PLOT + WEB\n" + "="*70)
print(f"‚úÖ Database: Connected (99K+ orders, 32K+ products)")
print(f"{'‚úÖ' if vectorstores else '‚ùå'} ChromaDB: {len(vectorstores) if vectorstores else 0} collections")
print(f"‚úÖ Gemini: 2.0 Flash")
print(f"‚úÖ Plot Agent: Matplotlib + Seaborn")
print(f"‚úÖ Web Agent: Ready")
show_help()

# ==================== INTERACTIVE LOOP ====================
while True:
    try:
        user_input = input("üí¨ You: ").strip()
        if not user_input:
            continue

        cmd = user_input.lower()
        if cmd in ['quit', 'exit', 'q']:
            print("\nüëã Goodbye!\n")
            break
        elif cmd == 'history':
            show_history()
        elif cmd == 'clear':
            clear_history()
        elif cmd == 'help':
            show_help()
        else:
            ask(user_input, delay=1.5)

    except KeyboardInterrupt:
        print("\n\nüëã Goodbye!\n")
        break
    except Exception as e:
        print(f"‚ùå Error: {e}\n")